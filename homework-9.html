<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Homework 9 — Foundations of Probability Theory</title>
  <link rel="stylesheet" href="style.css"/>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600;700&family=Orbitron:wght@600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$']],
        displayMath: [['$$', '$$']],
        processEscapes: true,
        processEnvironments: true
      },
      svg: { fontCache: 'global' },
      startup: {
        delay: 500,
        pageReady: () => MathJax.typesetPromise().catch(err => console.log(err))
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    main {
      max-width: 1500px !important;
      margin: 0 auto;
      padding: 0 1rem;
    }

    .hw-detail {
      max-width: 100%;
      width: 100%;
    }

    .hw-section {
      background: rgba(30,44,74,0.95);
      border-radius: 16px;
      margin: 2rem 0;
      padding: 2rem;
      box-shadow: 0 8px 32px rgba(24,224,230,0.15);
    }

    .subsection {
      background: rgba(20,32,66,0.6);
      border-radius: 12px;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-left: 4px solid #18e0e6;
    }

    .subsection-title {
      color: #18e0e6;
      font-weight: 600;
      font-size: 1.15rem;
      margin-bottom: 1rem;
    }

    .narrative {
      background: rgba(15,25,55,0.7);
      border-radius: 8px;
      padding: 1.25rem;
      margin: 1rem 0;
      border-left: 3px solid #4a9eff;
      font-size: 0.95rem;
      line-height: 1.8;
      color: #b8d4ff;
    }

    .math-box {
      background: rgba(8,15,35,0.95);
      border-radius: 10px;
      padding: 1.5rem;
      margin: 1rem 0;
      border-left: 3px solid #18e0e6;
      color: #e9f2ff;
      overflow-x: auto;
    }

    .formula-title {
      color: #18e0e6;
      font-weight: bold;
      margin-bottom: 0.75rem;
      font-size: 1.05rem;
    }

    .formula-content {
      color: #b8d4ff;
      line-height: 2;
      margin: 0.5rem 0;
      font-size: 1rem;
    }

    .formula-explanation {
      color: #a5d6a7;
      font-size: 0.95rem;
      margin-top: 0.75rem;
      line-height: 1.6;
      padding-top: 0.75rem;
      border-top: 1px solid rgba(165, 214, 167, 0.3);
    }

    .emphasis {
      color: #18e0e6;
      font-weight: 600;
      background: rgba(24, 224, 230, 0.1);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
    }

    .key-point {
      background: rgba(255, 152, 0, 0.1);
      border-left: 4px solid #ff9800;
      padding: 1rem;
      margin: 1rem 0;
      border-radius: 6px;
      color: #ffe0b2;
    }

    .key-point-title {
      color: #ffb74d;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }

    .diagram-box {
      background: rgba(8,15,35,0.9);
      border-radius: 10px;
      padding: 2rem;
      margin: 1.5rem 0;
      border: 2px solid #2d4a7a;
      text-align: center;
      font-family: 'Courier New', monospace;
      color: #18e0e6;
      overflow-x: auto;
      line-height: 1.8;
      font-size: 0.9rem;
    }

    .diagram-title {
      color: #18e0e6;
      font-weight: bold;
      margin-bottom: 1rem;
      font-size: 1.05rem;
    }

    .definition-box {
      background: rgba(76,175,80,0.08);
      border-left: 4px solid #81c784;
      border-radius: 8px;
      padding: 1.25rem;
      margin: 1rem 0;
      color: #a5d6a7;
    }

    .definition-title {
      color: #81c784;
      font-weight: 600;
      margin-bottom: 0.5rem;
      font-size: 1rem;
    }

    h1 {
      color: #18e0e6;
      font-size: 2.5rem;
      margin-bottom: 1rem;
      font-weight: 700;
      text-align: center;
    }

    h2 {
      color: #18e0e6;
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      font-weight: 600;
    }

    h3 {
      color: #4a9eff;
      font-size: 1.3rem;
      margin: 1.5rem 0 0.8rem 0;
      font-weight: 600;
    }

    body {
      background: linear-gradient(135deg, #0a1428 0%, #0f2847 100%);
      color: #b8d4ff;
      font-family: 'Montserrat', sans-serif;
      line-height: 1.7;
    }

    p {
      margin: 1rem 0;
      text-align: justify;
    }

    ul, ol {
      margin: 1rem 0;
      padding-left: 2rem;
      color: #b8d4ff;
    }

    li {
      margin: 0.5rem 0;
      line-height: 1.7;
    }
  </style>
</head>
<body>
 <header>
    <nav class="navbar">
      <div class="logo">Statistics<span class="cyber">Cyber</span></div>
      <ul id="navLinks">
        <li><a href="index.html">Home</a></li>
        <li><a href="homework.html">Homework</a></li>
        <li><a href="about.html">About</a></li>
      </ul>
      <button class="nav-toggle" id="navToggle">&#9776;</button>
    </nav>
  </header>


  <main>
    <div class="hw-detail">

      <h1>Homework 9: Foundations of Probability Theory</h1>

      <div class="narrative" style="text-align: center; font-size: 1rem; margin: 2rem 0;">
        A concise examination of probability theory's foundational concepts: the major interpretations of probability, Kolmogorov's axiomatic resolution, and connections to measure theory with essential derived properties.
      </div>

      <!-- SECTION 1: MAIN INTERPRETATIONS -->
      <div class="hw-section">
        <h2>1. Main Interpretations of Probability</h2>

        <div class="subsection">
          <div class="subsection-title">Classical Interpretation</div>
          <div class="narrative">
            The classical interpretation defines probability through a fundamental principle: when all outcomes are <span class="emphasis">equally likely</span>, the probability of an event is the ratio of favorable to total outcomes.
          </div>
          <div class="math-box">
            <div class="formula-title">Classical Probability Formula:</div>
            <div class="formula-content">
              $$P(A) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}}$$
            </div>
            <div class="formula-explanation">
              This formula applies when all outcomes have equal likelihood, such as rolling a fair die. For example, $P(\text{rolling a 3}) = \frac{1}{6}$.
            </div>
          </div>
          <div class="narrative">
            However, the classical interpretation has <span class="emphasis">fundamental limitations</span>: it assumes outcomes are "equally likely" (circular reasoning), and it only applies to <span class="emphasis">finite sample spaces</span>. Many real-world experiments have infinitely many outcomes or non-symmetric distributions where this approach fails entirely.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Frequentist Interpretation</div>
          <div class="narrative">
            The frequentist view grounds probability in empirical observation: the probability of an event is the <span class="emphasis">limit of its relative frequency</span> as an experiment is repeated infinitely many times under identical conditions.
          </div>
          <div class="math-box">
            <div class="formula-title">Frequentist Probability as Limiting Frequency:</div>
            <div class="formula-content">
              $$P(A) = \lim_{n \to \infty} \frac{n_A}{n}$$
            </div>
            <div class="formula-explanation">
              where $n_A$ is the number of times event $A$ occurs in $n$ repeated trials. For example, if a coin shows heads in 501 out of 1000 flips, we estimate $P(\text{heads}) \approx 0.501$.
            </div>
          </div>
          <div class="narrative">
            While this removes the equal-likelihood assumption and provides operational clarity, the frequentist approach faces <span class="emphasis">critical challenges</span>: the mathematical limit may not exist, and many important events are <span class="emphasis">inherently unrepeatable</span> (tomorrow's weather, a unique historical event, a specific business venture's success). The framework fundamentally struggles with single, non-repeatable phenomena.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Bayesian Interpretation</div>
          <div class="narrative">
            The Bayesian perspective treats probability as a measure of <span class="emphasis">subjective uncertainty</span> about a proposition. Rather than requiring repeated experiments, it applies to <span class="emphasis">any event</span>, including unique and unrepeatable ones, by systematically incorporating evidence through Bayes' theorem.
          </div>
          <div class="math-box">
            <div class="formula-title">Bayes' Theorem:</div>
            <div class="formula-content">
              $$P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}$$
            </div>
            <div class="formula-explanation">
              <strong>Components:</strong>
              <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
                <li>$P(H)$ = prior probability of hypothesis $H$ (initial belief before evidence)</li>
                <li>$P(E|H)$ = likelihood of evidence $E$ given hypothesis $H$ is true</li>
                <li>$P(E)$ = total probability of evidence (normalizing constant)</li>
                <li>$P(H|E)$ = <span class="emphasis">posterior</span> probability of $H$ given $E$ (updated belief)</li>
              </ul>
            </div>
          </div>
          <div class="narrative">
            The Bayesian framework is <span class="emphasis">uniquely powerful</span> for applying probability to unrepeatable events: a medical diagnosis, a defendant's guilt, or the probability a historical figure acted for certain motivations can all be assigned meaningful probabilities by encoding available information. However, the subjectivity inherent in choosing prior beliefs is contentious—different individuals with different priors may assign different probabilities to the same event.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Geometric Interpretation</div>
          <div class="narrative">
            The geometric interpretation defines probability for uniformly distributed points: when a random point is distributed uniformly within a region, the probability it falls in a sub-region equals the <span class="emphasis">ratio of measures</span> (length, area, volume).
          </div>
          <div class="math-box">
            <div class="formula-title">Geometric Probability:</div>
            <div class="formula-content">
              $$P(A) = \frac{\text{Measure}(A)}{\text{Measure}(\Omega)}$$
            </div>
            <div class="formula-explanation">
              For instance, if a dart is thrown uniformly at a circle of radius 1, and we want the probability it lands within distance 0.5 from center: $P = \frac{\text{Area of inner circle}}{\text{Area of full circle}} = \frac{\pi(0.5)^2}{\pi(1)^2} = 0.25$.
            </div>
          </div>
          <div class="narrative">
            This provides intuitive visual understanding and handles continuous spaces naturally. However, it assumes <span class="emphasis">uniform distribution</span> (another implicit assumption) and only applies to well-behaved, finite measure spaces.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Conceptual Tensions</div>
          <div class="narrative">
            These interpretations conflict on fundamental questions: <span class="emphasis">What events can have probabilities?</span> Classical and geometric interpretations require finite, equally-likely outcomes. Frequentist interpretation requires repeatable trials. Bayesian interpretation applies to any proposition. <span class="emphasis">Are probabilities objective or subjective?</span> Classical and frequentist are objective; Bayesian is subjective. <span class="emphasis">How are probabilities determined?</span> Different methods for each. These are not merely philosophical quibbles—they produce genuinely different numerical answers for the same scenario.
          </div>
          <div class="key-point">
            <div class="key-point-title">The Core Conflict:</div>
            For over a century, these competing interpretations created genuine confusion in probability's foundations. Mathematicians and philosophers disagreed fundamentally on what probability "really is," fragmenting the discipline conceptually. This was not merely unsatisfying intellectually—it created practical uncertainty about the logical status of probability as a mathematical discipline.
          </div>
        </div>
      </div>

      <!-- SECTION 2: AXIOMATIC APPROACH -->
      <div class="hw-section">
        <h2>2. Kolmogorov's Axiomatic Approach & Resolution</h2>

        <div class="narrative">
          Rather than declare one interpretation "correct," Andrey Kolmogorov (1933) revolutionized probability by <span class="emphasis">shifting focus entirely</span>: instead of asking "<span class="emphasis">what is probability?</span>", ask "<span class="emphasis">what properties must probability have?</span>" By identifying minimal essential mathematical properties as axioms, he unified all interpretations under a single structure. Classical, frequentist, Bayesian, and geometric probabilities all satisfy the axioms, making them mathematically valid despite interpretational differences.
        </div>

        <div class="subsection">
          <div class="subsection-title">The Three Axioms of Probability</div>

          <div class="math-box">
            <div class="formula-title">Axiom 1: Non-negativity</div>
            <div class="formula-content">
              $$P(A) \geq 0 \quad \text{for all events } A \in \mathcal{F}$$
            </div>
            <div class="formula-explanation">
              Probabilities cannot be negative. This reflects the fundamental intuition that probability measures the <span class="emphasis">degree of likelihood</span>, which inherently cannot be negative.
            </div>
          </div>

          <div class="math-box">
            <div class="formula-title">Axiom 2: Normalization (Unit Measure)</div>
            <div class="formula-content">
              $$P(\Omega) = 1$$
            </div>
            <div class="formula-explanation">
              The sample space $\Omega$ (the set of <span class="emphasis">all possible outcomes</span>) has total probability exactly 1. This encodes the certainty that some outcome must occur—one outcome is certain to happen. This normalization ensures probabilities are comparably interpretable as values between 0 and 1.
            </div>
          </div>

          <div class="math-box">
            <div class="formula-title">Axiom 3: Countable Additivity</div>
            <div class="formula-content">
              $$P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \quad \text{for disjoint events}$$
            </div>
            <div class="formula-explanation">
              For any countably infinite collection of <span class="emphasis">mutually disjoint</span> (non-overlapping) events $A_1, A_2, A_3, \ldots$, the probability that at least one occurs equals the sum of their individual probabilities. For finite disjoint events: $P(A \cup B) = P(A) + P(B)$ when $A \cap B = \emptyset$.
            </div>
          </div>

          <div class="key-point">
            <div class="key-point-title">Why These Axioms?</div>
            These axioms are <span class="emphasis">minimal</span>—they impose the fewest constraints necessary for consistent probability theory. They encode essential intuitions about probability without imposing a particular interpretation. Any assignment of numbers to events satisfying these three axioms is mathematically valid probability, regardless of interpretation.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">How the Axioms Resolve Conflicts</div>
          <div class="narrative">
            <strong>Single vs. Repeatable Events:</strong> The axioms impose no restrictions on event types. The axioms never state "probability only applies to repeatable experiments." Any assignment satisfying the three axioms is valid—whether for repeatable experiments or unique events. A Bayesian can assign probability 0.6 to a unique historical event, and this is mathematically valid if it satisfies the axioms.
          </div>
          <div class="narrative">
            <strong>Objectivity vs. Subjectivity:</strong> Both are mathematically valid. A frequentist's empirically-derived probability based on 10,000 trials satisfies the axioms. A Bayesian's subjective belief reflecting personal assessment also satisfies the axioms. The axioms are neutral on whether probabilities are "really" objective or subjective.
          </div>
          <div class="narrative">
            <strong>Finite vs. Infinite Spaces:</strong> Countable additivity (Axiom 3) naturally generalizes to <span class="emphasis">infinite collections</span> of events, encompassing both discrete and continuous probability spaces. The axioms provide unified language for finite spaces (discrete probability) and infinite spaces (continuous probability).
          </div>
          <div class="key-point">
            <div class="key-point-title">Core Resolution Principle:</div>
            By shifting from "<span class="emphasis">what does probability mean?</span>" to "<span class="emphasis">what must probability satisfy?</span>", the axioms bypass philosophical disputes. Different interpretations answer the first question differently but <span class="emphasis">all agree on the second</span>. The mathematical framework is interpretation-agnostic. This is a feature, not a flaw—it allows mathematicians to work rigorously while remaining genuinely agnostic about ultimate philosophical nature.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">The Probability Space Structure</div>
          <div class="narrative">
            A probability space is the complete formal structure incarnating Kolmogorov's axioms. It is a triplet <span class="emphasis">$(\Omega, \mathcal{F}, P)$</span> consisting of three precisely defined components:
          </div>
          <div class="definition-box">
            <div class="definition-title">• Sample Space $\Omega$</div>
            The set of <span class="emphasis">all possible outcomes</span> of the experiment. Must be non-empty and exhaustive—every conceivable outcome is included. Example: $\Omega = \{1, 2, 3, 4, 5, 6\}$ for a die roll.
          </div>
          <div class="definition-box">
            <div class="definition-title">• σ-algebra $\mathcal{F}$</div>
            A collection of subsets of $\Omega$ representing <span class="emphasis">which subsets qualify as events</span>. Must contain $\emptyset$ and $\Omega$, be closed under complementation (if $A$ is an event, so is $A^c$), and closed under countable unions (if $A_1, A_2, \ldots$ are events, their union is an event).
          </div>
          <div class="definition-box">
            <div class="definition-title">• Probability Measure $P$</div>
            A function $P: \mathcal{F} \to [0,1]$ assigning a probability to each event, satisfying Kolmogorov's three axioms. This is the actual rule that maps events to numbers.
          </div>
          <div class="narrative" style="margin-top: 1.5rem;">
            By explicitly specifying all three components, a probability space makes clear: what outcomes exist, which subsets are "valid events," and what probabilities are assigned. <span class="emphasis">Different models</span> of the same phenomenon may use different probability spaces (Bayesian vs. frequentist), yet both satisfy the axioms and are mathematically valid. The difference is interpretational, not mathematical.
          </div>
        </div>
      </div>

      <!-- SECTION 3: MEASURE THEORY CONNECTION -->
      <div class="hw-section">
        <h2>3. Probability & Measure Theory</h2>

        <div class="narrative">
          Probability theory is <span class="emphasis">not exotic or separate</span> from broader mathematics—it is a natural <span class="emphasis">specialization of measure theory</span>. A measure is a function assigning "size" (length, area, volume) to abstract sets. Probability measures are measures where the total "size" is normalized to 1. This profound unification reveals that probability inherits powerful mathematical tools.
        </div>

        <div class="subsection">
          <div class="subsection-title">σ-algebras and Measurable Events</div>
          <div class="narrative">
            A σ-algebra defines which subsets of $\Omega$ are "measurable"—which subsets can meaningfully be assigned a measure. Why necessary? <span class="emphasis">For uncountable spaces like the real line, assigning a measure to every possible subset leads to paradoxes.</span> σ-algebras restrict attention to a carefully chosen collection of "nice" subsets that can be consistently measured.
          </div>
          <div class="definition-box">
            <div class="definition-title">Properties of a σ-algebra $\mathcal{F}$:</div>
            <ul style="margin: 0.5rem 0;">
              <li>$\emptyset \in \mathcal{F}$ (contains the empty set, impossible event)</li>
              <li>If $A \in \mathcal{F}$, then $A^c \in \mathcal{F}$ (closed under complementation)</li>
              <li>If $\{A_1, A_2, \ldots\} \subseteq \mathcal{F}$, then $\bigcup_{i=1}^{\infty} A_i \in \mathcal{F}$ (closed under countable unions)</li>
            </ul>
          </div>
          <div class="narrative">
            For finite spaces, the σ-algebra can be the <span class="emphasis">power set</span> (all possible subsets). For continuous spaces like $\mathbb{R}$, the <span class="emphasis">Borel σ-algebra</span> (generated by open intervals) is standard—rich enough for practical use, restricted enough to avoid paradoxes.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Random Variables and Measurability</div>
          <div class="narrative">
            A random variable is a function $X: \Omega \to \mathbb{R}$ mapping outcomes to real numbers. For probabilities to be <span class="emphasis">well-defined</span> for events involving $X$, the function must satisfy a measurability condition: if $B$ is a standard set of real numbers (Borel set), then the <span class="emphasis">preimage</span> $X^{-1}(B) = \{\omega \in \Omega : X(\omega) \in B\}$ must belong to $\mathcal{F}$.
          </div>
          <div class="math-box">
            <div class="formula-title">Measurability Requirement:</div>
            <div class="formula-content">
              $$\text{If } B \subseteq \mathbb{R} \text{ is Borel}, \quad \text{then } X^{-1}(B) \in \mathcal{F}$$
            </div>
            <div class="formula-explanation">
              This ensures that probabilities like $P(X \leq 3)$ or $P(X \in [0,1])$ are well-defined as events in our σ-algebra. Without measurability, random variables could be defined in ways incompatible with the probability structure, leading to undefined or contradictory probabilities.
            </div>
          </div>
          <div class="narrative">
            In practice, measurability holds automatically for typical spaces and functions. However, the condition is <span class="emphasis">mathematically essential</span>—it prevents pathological situations and ensures logical consistency throughout probability theory.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Connection to Measure Theory</div>
          <div class="narrative">
            In measure theory, a general measure $\mu$ on $(Ω, \mathcal{F})$ satisfies:
          </div>
          <div class="math-box">
            <div class="formula-title">Measure Properties:</div>
            <div class="formula-content">
              $$\mu(\emptyset) = 0 \quad \text{(empty set has zero measure)}$$
              $$\mu\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i) \quad \text{(countable additivity)}$$
            </div>
            <div class="formula-explanation">
              A probability measure is a <span class="emphasis">specialized measure</span> where $P(\Omega) = 1$. The normalization is the only distinction. This reveals probability is not a separate concept—it is measure theory applied to events, with total measure normalized to 1.
            </div>
          </div>
          <div class="narrative">
            This unification is <span class="emphasis">profoundly consequential</span>: techniques from measure theory—Lebesgue integration, convergence theorems, Fubini's theorem for products—automatically apply to probability. The Radon-Nikodym theorem, a pillar of measure theory, characterizes when probability distributions possess densities. Discrete and continuous probability are not fundamentally different; they are <span class="emphasis">instantiations of the same measure-theoretic structure</span>, differing only in the geometry of the space.
          </div>
        </div>
      </div>

      <!-- SECTION 4: FUNDAMENTAL PROPERTIES -->
      <div class="hw-section">
        <h2>4. Fundamental Properties Derived from the Axioms</h2>

        <div class="subsection">
          <div class="subsection-title">Subadditivity (Union Bound)</div>
          <div class="narrative">
            Subadditivity is a foundational property that immediately follows from the axioms. It establishes that the probability of a union cannot exceed the sum of individual probabilities—a powerful tool when exact computation is difficult.
          </div>
          <div class="math-box">
            <div class="formula-title">Subadditivity / Union Bound:</div>
            <div class="formula-content">
              $$P(A_1 \cup A_2 \cup \cdots \cup A_n) \leq P(A_1) + P(A_2) + \cdots + P(A_n)$$
            </div>
            <div class="formula-explanation">
              When events may overlap, adding probabilities without accounting for overlaps causes overcounting. The union's probability is bounded above by the sum. <strong>Example:</strong> If event $A$ has probability 0.3 and event $B$ has probability 0.4, then $P(A \cup B) \leq 0.7$. If they overlap significantly, $P(A \cup B)$ could be much less—say, 0.5.
            </div>
          </div>
          <div class="narrative">
            <span class="emphasis">Equality holds exactly when events are disjoint</span> (mutually exclusive). When events overlap, there is <span class="emphasis">strict inequality</span>—the overlapping region is counted once in the union but twice in the sum of probabilities. The union bound is invaluable in statistical testing (controlling for multiple comparisons) and reliability engineering (bounding failure probabilities).
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Inclusion–Exclusion Principle</div>
          <div class="narrative">
            While subadditivity provides <span class="emphasis">conservative bounds</span>, inclusion-exclusion yields <span class="emphasis">exact formulas</span> by systematically accounting for all overlaps through alternating sums of intersection probabilities. This is fundamental in combinatorics and probability for exact answers.
          </div>
          <div class="math-box">
            <div class="formula-title">Inclusion-Exclusion for Two Events:</div>
            <div class="formula-content">
              $$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
            </div>
            <div class="formula-explanation">
              The third term $-P(A \cap B)$ corrects for <span class="emphasis">overcounting the overlap</span>. When we add $P(A) + P(B)$, the intersection region is counted twice, so we subtract it once. <strong>Example:</strong> If $P(A) = 0.3$, $P(B) = 0.4$, and $P(A \cap B) = 0.1$, then $P(A \cup B) = 0.3 + 0.4 - 0.1 = 0.6$.
            </div>
          </div>

          <div class="math-box">
            <div class="formula-title">Inclusion-Exclusion for Three Events:</div>
            <div class="formula-content">
              $$P(A \cup B \cup C) = P(A) + P(B) + P(C)$$
              $$- P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$$
            </div>
            <div class="formula-explanation">
              <strong>Pattern explanation:</strong> Include all individual probabilities (first line), exclude pairwise intersections (they were counted twice in the sum, so subtract once), then include triple intersections back (they were subtracted three times, so add once). This alternating pattern perfectly corrects for all overlaps at each level. The pattern continues for arbitrarily many events.
            </div>
          </div>

          <div class="narrative">
            The principle is essential for exact answers: the probability of at least one error in a communication system, at least one person sharing a birthday in a group, or at least one component failure in a system. Each term corrects for over- or under-counting at that overlap level, yielding an exact formula.
          </div>
        </div>

        <div class="subsection">
          <div class="subsection-title">Other Essential Properties</div>

          <div class="math-box">
            <div class="formula-title">Complement Rule:</div>
            <div class="formula-content">
              $$P(A^c) = 1 - P(A)$$
            </div>
            <div class="formula-explanation">
              Since $A$ and its complement $A^c$ partition the entire sample space (they cover everything with no overlap), their probabilities must sum to 1. This is immediate from Axiom 2.
            </div>
          </div>

          <div class="math-box">
            <div class="formula-title">Monotonicity:</div>
            <div class="formula-content">
              $$\text{If } A \subseteq B, \text{ then } P(A) \leq P(B)$$
            </div>
            <div class="formula-explanation">
              If $A$ occurring implies $B$ must occur, then $B$ is "at least as likely" as $A$. Intuitively: a more restrictive condition cannot be more probable than a less restrictive one.
            </div>
          </div>

          <div class="math-box">
            <div class="formula-title">Null Event:</div>
            <div class="formula-content">
              $$P(\emptyset) = 0$$
            </div>
            <div class="formula-explanation">
              The empty set (impossible event) has zero probability. This follows from Axiom 2: since $\Omega = \Omega \cup \emptyset$ (empty union doesn't add anything), we have $P(\Omega) = P(\Omega) + P(\emptyset)$, so $P(\emptyset) = 0$.
            </div>
          </div>
        </div>
      </div>

    </div>
  </main>

<footer>
    <span>&copy; 2025 Lorenzo Ciafrelli | MS Cybersecurity, Sapienza University of Rome</span>
  </footer>
</body>
</html>